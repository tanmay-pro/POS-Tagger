{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import conllu\n",
    "from tqdm import tqdm, notebook\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, classification_report\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used device: GPU\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('Used device: GPU')\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print('Used device: CPU')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFile = \"dataset/train.conllu\"\n",
    "devFile = \"dataset/dev.conllu\"\n",
    "testFile = \"dataset/test.conllu\"\n",
    "\n",
    "f = open(trainFile, \"r\", encoding=\"utf-8\")\n",
    "trainData = conllu.parse(f.read())\n",
    "\n",
    "f = open(devFile, \"r\", encoding=\"utf-8\")\n",
    "devData = conllu.parse(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class obsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, inpData):\n",
    "        self.word2idx = {\"UNK\" : 0}\n",
    "        self.idx2word = [\"UNK\"]\n",
    "        self.idTensor = []\n",
    "        self.vocabSize = 0\n",
    "        self.unkownWords = []\n",
    "        self.data = self.makeDict(inpData)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = len(self.idx2word)\n",
    "            self.idx2word.append(word)\n",
    "        return self.word2idx[word]\n",
    "    \n",
    "    def getWord(self, idx):\n",
    "        return self.idx2word[idx]\n",
    "\n",
    "    def makeDict(self, data):\n",
    "        totalWords = 0\n",
    "        for sent in data:\n",
    "            totalWords += len(sent)\n",
    "            for word in sent:\n",
    "                self.addWord(word[\"form\"])\n",
    "        \n",
    "        # make words unkown if freq < 3\n",
    "        wordFreq = {}\n",
    "        for sent in data:\n",
    "            for word in sent:\n",
    "                if word[\"form\"] not in wordFreq:\n",
    "                    wordFreq[word[\"form\"]] = 1\n",
    "                else:\n",
    "                    wordFreq[word[\"form\"]] += 1\n",
    "        \n",
    "        for word in wordFreq:\n",
    "            if wordFreq[word] < 3:\n",
    "                self.word2idx[word] = 0\n",
    "                self.unkownWords.append(word)\n",
    "                \n",
    "                \n",
    "        for i in range(len(data)):\n",
    "            tempTensor = torch.zeros(len(data[i]), dtype=torch.long)\n",
    "            for j in range(len(data[i])):\n",
    "                tempTensor[j] = self.word2idx[data[i][j][\"form\"]]\n",
    "            self.idTensor.append(tempTensor)\n",
    "\n",
    "        self.vocabSize = len(self.idx2word)\n",
    "        \n",
    "class stateDataset(Dataset):\n",
    "\n",
    "    def __init__(self, inpData, unkownWords):\n",
    "        self.word2idx = {\"UNK\" : 0}\n",
    "        self.idx2word = [\"UNK\"]\n",
    "        self.idTensor = []\n",
    "        self.vocabSize = 0\n",
    "        self.unkownWords = unkownWords\n",
    "        self.data = self.makeDict(inpData)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = len(self.idx2word)\n",
    "            self.idx2word.append(word)\n",
    "        return self.word2idx[word]\n",
    "    \n",
    "    def getWord(self, idx):\n",
    "        return self.idx2word[idx]\n",
    "\n",
    "    def makeDict(self, data):\n",
    "        totalWords = 0\n",
    "        for sent in data:\n",
    "            totalWords += len(sent)\n",
    "            for word in sent:\n",
    "                self.addWord(word[\"upos\"])\n",
    "\n",
    "        # if word is unknown take its pos tag as unkown\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            tempTensor = torch.zeros(len(data[i]), dtype=torch.long)\n",
    "            for j in range(len(data[i])):\n",
    "                if data[i][j][\"form\"] in self.unkownWords:\n",
    "                    tempTensor[j] = self.word2idx[\"UNK\"]\n",
    "                else:\n",
    "                    tempTensor[j] = self.word2idx[data[i][j][\"upos\"]]\n",
    "            \n",
    "            self.idTensor.append(tempTensor)\n",
    "        \n",
    "        self.vocabSize = len(self.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padSent(currBatch):\n",
    "    currBatch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    maxLen = len(currBatch[0][0])\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for i in range(len(currBatch)):\n",
    "        currBatch[i][0] = torch.cat((currBatch[i][0], torch.zeros(maxLen - len(currBatch[i][0]), dtype=torch.long)))\n",
    "        currBatch[i][1] = torch.cat((currBatch[i][1], torch.zeros(maxLen - len(currBatch[i][1]), dtype=torch.long)))\n",
    "        inputs.append(currBatch[i][0])\n",
    "        targets.append(currBatch[i][1])\n",
    "    inputs = torch.stack(inputs)\n",
    "    targets = torch.stack(targets)\n",
    "    return inputs, targets\n",
    "\n",
    "def createLoaders(trainData, devData):\n",
    "    trainObj1 = obsDataset(trainData)\n",
    "    unkownWords = trainObj1.unkownWords\n",
    "    trainObj2 = stateDataset(trainData, unkownWords)\n",
    "    combined = []\n",
    "    for i in range(len(trainObj1.idTensor)):\n",
    "        combined.append([trainObj1.idTensor[i], trainObj2.idTensor[i]])\n",
    "    trainLoader = DataLoader(combined, batch_size, shuffle=True, collate_fn=padSent)\n",
    "\n",
    "    devObj1 = obsDataset(devData)\n",
    "    unkown2 = devObj1.unkownWords\n",
    "    devObj2 = stateDataset(devData, unkown2)\n",
    "    combined2 = []\n",
    "    for i in range(len(devObj1.idTensor)):\n",
    "        combined2.append([devObj1.idTensor[i], devObj2.idTensor[i]])\n",
    "    devLoader = DataLoader(combined2, batch_size, shuffle=True, collate_fn=padSent)\n",
    "    return trainObj1, trainObj2, trainLoader, devLoader, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, inputVocabSize, outputVocabSize, num_layers):\n",
    "        super(GRU, self).__init__()\n",
    "        self.encoding = nn.Embedding(inputVocabSize, embedding_size)\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, num_layers, batch_first=True, dropout=0.4)\n",
    "        self.decoding = nn.Linear(hidden_size, outputVocabSize)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.activation = nn.LogSoftmax(dim=2)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        batch_size, seq_len = input.size()\n",
    "        embed = self.encoding(input)\n",
    "        seqTensor = torch.tensor([seq_len] * batch_size, dtype=torch.long)\n",
    "        updatedEmbedding = pack_padded_sequence(embed, seqTensor, batch_first=True)\n",
    "        output, hidden = self.gru(updatedEmbedding, hidden)\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        output = self.decoding(output)\n",
    "        output = self.activation(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, trainLoader, num_epochs, learning_rate, devLoader, outputVocabSize):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    counter = 0\n",
    "    lossArray = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (inputs, targets) in enumerate(trainLoader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            hidden = torch.zeros(num_layers, inputs.size(0), hidden_size)\n",
    "            hidden = hidden.to(device)\n",
    "            output, hidden = model(inputs, hidden)\n",
    "            loss = model.loss(output.view(-1, outputVocabSize), targets.view(-1))\n",
    "            lossArray.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if counter % 20 == 0:\n",
    "                devLossArray = []\n",
    "                for i, (inputs, targets) in enumerate(devLoader):\n",
    "                    inputs = inputs.to(device)\n",
    "                    targets = targets.to(device)\n",
    "                    hidden = torch.zeros(num_layers, inputs.size(0), hidden_size)\n",
    "                    hidden = hidden.to(device)\n",
    "                    output, hidden = model(inputs, hidden)\n",
    "                    loss = model.loss(output.view(-1, outputVocabSize), targets.view(-1))\n",
    "                    devLossArray.append(loss.item())\n",
    "                avgTrainLoss = sum(lossArray) / len(lossArray)\n",
    "                avgDevLoss = sum(devLossArray) / len(devLossArray)\n",
    "                print(\"Epoch: \", epoch, \" Train Loss: \", avgTrainLoss, \" Dev Loss: \", avgDevLoss)\n",
    "                lossArray = []\n",
    "            counter += 1\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reportTrainingAccuracy(model, trainLoader, outputVocabSize):\n",
    "    yTrue = []\n",
    "    yPred = []\n",
    "    for x, y in trainLoader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        hidden = torch.zeros(num_layers, x.size(0), hidden_size).to(device)\n",
    "        output, hidden = model(x, hidden)\n",
    "        output = output.view(-1, outputVocabSize)\n",
    "        y = y.view(-1)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        yTrue.extend(y.tolist())\n",
    "        yPred.extend(predicted.tolist())\n",
    "    y_true = np.array(yTrue)\n",
    "    y_pred = np.array(yPred)\n",
    "    print(\"Training Accuracy: \", accuracy_score(y_true, y_pred))\n",
    "    f1scores  = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    print(\"Training Classification Report: \")\n",
    "    print(\"Training F1 Macro Score: \", f1scores)\n",
    "    print(classification_report(yTrue, yPred, target_names=trainObj2.idx2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testData(model, testFile, outputVocabSize, trainingVocab, tagw2i):\n",
    "    f = open(testFile, \"r\", encoding=\"utf-8\")\n",
    "    testData = conllu.parse(f.read())\n",
    "\n",
    "    trainingVocab = trainingVocab\n",
    "    yTrue = []\n",
    "    yPred = []\n",
    "    for sent in testData:\n",
    "        for word in sent:\n",
    "            if word[\"form\"] not in trainingVocab:\n",
    "                word[\"form\"] = \"UNK\"\n",
    "                word[\"upos\"] = \"UNK\"\n",
    "        idTensor = []\n",
    "        for word in sent:\n",
    "            idTensor.append(trainingVocab[word[\"form\"]])\n",
    "        idTensor = torch.tensor(idTensor, dtype=torch.long).to(device)\n",
    "        hidden = torch.zeros(num_layers, 1, hidden_size).to(device)\n",
    "        output, hidden = model(idTensor.view(1, -1), hidden)\n",
    "        output = output.view(-1, outputVocabSize)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        predicted = predicted.tolist()\n",
    "        yPred.extend(predicted)\n",
    "        yTrue.extend([tagw2i[word[\"upos\"]] for word in sent])\n",
    "    yTrue = np.array(yTrue)\n",
    "    yPred = np.array(yPred)\n",
    "    # use classification report to report accuracy, precision and f1 score\n",
    "    print(\"Testing Accuracy: \", accuracy_score(yTrue, yPred))\n",
    "    f1scores  = f1_score(yTrue, yPred, average=\"macro\")\n",
    "    print(\"Testing F1 Macro Score: \", f1scores)\n",
    "    print(\"Testing Classification Report: \")\n",
    "    print(classification_report(yTrue, yPred, target_names=trainObj2.idx2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 28\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "batch_size = 32\n",
    "num_epochs = 15\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainObj1, trainObj2, trainLoader, devLoader, combined = createLoaders(trainData, devData)\n",
    "model = GRU(embedding_size, hidden_size, trainObj1.vocabSize, trainObj2.vocabSize, num_layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainModel(model, trainLoader, num_epochs, learning_rate, devLoader, trainObj2.vocabSize)\n",
    "torch.save(model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9957427112368009\n",
      "Training Classification Report: \n",
      "Training F1 Macro Score:  0.972844266620372\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         UNK       1.00      1.00      1.00    148388\n",
      "        PRON       0.89      0.99      0.94      3018\n",
      "         AUX       0.98      0.98      0.98      1729\n",
      "         DET       1.00      0.90      0.94      3804\n",
      "        NOUN       1.00      1.00      1.00      8535\n",
      "         ADP       1.00      0.99      0.99     10781\n",
      "       PROPN       1.00      1.00      1.00     11581\n",
      "        VERB       0.96      0.99      0.97      4531\n",
      "         NUM       1.00      1.00      1.00       801\n",
      "         ADJ       0.96      0.95      0.96      1602\n",
      "       CCONJ       1.00      1.00      1.00       750\n",
      "         ADV       0.97      0.90      0.93       402\n",
      "        PART       0.90      0.97      0.94       366\n",
      "        INTJ       0.97      0.99      0.98       316\n",
      "\n",
      "    accuracy                           1.00    196604\n",
      "   macro avg       0.97      0.97      0.97    196604\n",
      "weighted avg       1.00      1.00      1.00    196604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reportTrainingAccuracy(model, trainLoader, trainObj2.vocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy:  0.965501519756839\n",
      "Testing F1 Macro Score:  0.9235152006253741\n",
      "Testing Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         UNK       0.43      1.00      0.61        43\n",
      "        PRON       0.85      0.98      0.91       392\n",
      "         AUX       0.97      0.98      0.98       256\n",
      "         DET       0.97      0.87      0.92       512\n",
      "        NOUN       0.99      0.98      0.99      1164\n",
      "         ADP       1.00      0.99      0.99      1434\n",
      "       PROPN       0.99      0.98      0.99      1555\n",
      "        VERB       0.98      0.94      0.96       622\n",
      "         NUM       0.95      0.91      0.93       107\n",
      "         ADJ       0.89      0.95      0.91       220\n",
      "       CCONJ       0.99      0.97      0.98       109\n",
      "         ADV       0.96      0.70      0.81        74\n",
      "        PART       0.98      0.96      0.97        56\n",
      "        INTJ       1.00      0.97      0.99        36\n",
      "\n",
      "    accuracy                           0.97      6580\n",
      "   macro avg       0.93      0.94      0.92      6580\n",
      "weighted avg       0.97      0.97      0.97      6580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testData(model, testFile, trainObj2.vocabSize, trainObj1.word2idx, trainObj2.word2idx)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "304bbaea038905129556f2fe4a4a34c7dbeddb641af9197964642f039e8dd09e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
